import openai
import streamlit as st
import re
import json
import tempfile
import os
import yt_dlp
import requests
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# Configure Streamlit page
st.set_page_config(
    page_title="Accent Detection App",
    page_icon="üé≠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add custom CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
    }
    .feature-found {
        background: #e8f5e8;
        padding: 0.2rem 0.5rem;
        border-radius: 4px;
        margin: 0.1rem;
        display: inline-block;
    }
</style>
""", unsafe_allow_html=True)

# Get API key from secrets
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")

def detect_accent_patterns(text: str, language_code: str) -> Dict:
    """Rule-based accent detection using linguistic patterns"""
    
    text_lower = text.lower()
    detected_features = []
    accent_scores = {}
    
    if language_code.startswith('en'):  # English language variants
        
        # American English indicators
        american_indicators = [
            ('color', 'favor', 'honor'),  # -or spellings
            ('ize', 'ization'),  # -ize endings
            ('center', 'theater'),  # -er endings
            ('mom', 'gotten', 'fall'),  # vocabulary
            ('elevator', 'apartment', 'candy'),
            ('truck', 'gas', 'sidewalk'),
            ('y\'all', 'gonna', 'wanna')  # contractions
        ]
        
        # British English indicators
        british_indicators = [
            ('colour', 'favour', 'honour'),  # -our spellings
            ('ise', 'isation'),  # -ise endings
            ('centre', 'theatre'),  # -re endings
            ('mum', 'autumn', 'lift'),  # vocabulary
            ('flat', 'biscuit', 'rubber'),
            ('lorry', 'petrol', 'pavement'),
            ('brilliant', 'mate', 'cheers'),
            ('whilst', 'amongst')  # formal variants
        ]
        
        # Australian English indicators
        australian_indicators = [
            ('arvo', 'barbie', 'brekkie'),  # abbreviations
            ('mate', 'sheila', 'bloke'),
            ('fair dinkum', 'no worries', 'too right'),
            ('uni', 'servo', 'bottle-o'),
            ('crikey', 'strewth')
        ]
        
        # Canadian English indicators
        canadian_indicators = [
            ('eh', 'about', 'house'),  # 'about' pronunciation markers in text
            ('toque', 'hydro', 'washroom'),
            ('double-double', 'loonie', 'toonie'),
            ('chesterfield', 'runners')
        ]
        
        # South African English indicators
        south_african_indicators = [
            ('braai', 'boet', 'howzit'),
            ('lekker', 'shame', 'eish'),
            ('robot', 'bakkie', 'kombi'),  # robot = traffic light
            ('now now', 'just now')
        ]
        
        # Score each accent
        accent_patterns = {
            'American': american_indicators,
            'British': british_indicators,
            'Australian': australian_indicators,
            'Canadian': canadian_indicators,
            'South African': south_african_indicators
        }
        
        for accent, patterns in accent_patterns.items():
            score = 0
            found_features = []
            
            for pattern_group in patterns:
                for pattern in pattern_group:
                    if pattern in text_lower:
                        score += 1
                        found_features.append(pattern)
            
            if score > 0:
                accent_scores[accent] = {
                    'score': score,
                    'features': found_features,
                    'confidence': min(score * 15, 85)  # Cap at 85%
                }
    
    # Handle other languages
    elif language_code.startswith('es'):  # Spanish
        spanish_regions = {
            'Mexican': ['√≥rale', 'g√ºey', 'chido', 'padre'],
            'Argentinian': ['che', 'boludo', 'b√°rbaro'],
            'Spanish (Spain)': ['vale', 't√≠o', 'guay', 'joder'],
            'Colombian': ['parce', 'chimba', 'bacano']
        }
        
        for region, indicators in spanish_regions.items():
            score = sum(1 for word in indicators if word in text_lower)
            if score > 0:
                accent_scores[region] = {
                    'score': score,
                    'features': [w for w in indicators if w in text_lower],
                    'confidence': min(score * 20, 80)
                }
    
    elif language_code.startswith('fr'):  # French
        french_regions = {
            'French (France)': ['putain', 'bordel', 'coucou'],
            'Canadian French': ['tabarnak', 'c√¢lisse', 'osti'],
            'Belgian French': ['nonante', 'septante']
        }
        
        for region, indicators in french_regions.items():
            score = sum(1 for word in indicators if word in text_lower)
            if score > 0:
                accent_scores[region] = {
                    'score': score,
                    'features': [w for w in indicators if w in text_lower],
                    'confidence': min(score * 25, 80)
                }
    
    # Return results
    if accent_scores:
        top_accent = max(accent_scores.items(), key=lambda x: x[1]['score'])
        return {
            'detected_accent': top_accent[0],
            'confidence': top_accent[1]['confidence'],
            'features_found': top_accent[1]['features'],
            'all_scores': accent_scores,
            'method': 'rule_based'
        }
    else:
        return {
            'detected_accent': 'Unknown',
            'confidence': 0,
            'features_found': [],
            'all_scores': {},
            'method': 'rule_based'
        }

def analyze_accent_with_openai(text: str, language_code: str) -> Dict:
    """Use OpenAI to analyze accent from transcribed text"""
    
    if not OPENAI_API_KEY:
        return {"error": "OpenAI API key not configured"}
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    
    # Language-specific prompts
    language_context = {
        'en': "English (considering American, British, Australian, Canadian, South African, Irish, etc.)",
        'es': "Spanish (considering Mexican, Argentinian, Spanish, Colombian, etc.)",
        'fr': "French (considering French, Canadian French, Belgian French, etc.)",
        'de': "German (considering German, Austrian, Swiss German, etc.)",
        'it': "Italian (considering standard Italian, regional variants)",
        'pt': "Portuguese (considering Brazilian, European Portuguese)"
    }
    
    lang_context = language_context.get(language_code[:2], f"Language code: {language_code}")
    
    prompt = f"""
    Analyze the following transcribed speech text and identify the likely accent/dialect for {lang_context}:
    
    Text: "{text}"
    
    Please provide a JSON response with:
    {{
        "primary_accent": "most likely accent/dialect",
        "confidence": confidence_percentage_as_number,
        "key_indicators": ["list", "of", "specific", "words", "or", "phrases"],
        "alternative_accents": [
            {{"accent": "alternative1", "probability": percentage}},
            {{"accent": "alternative2", "probability": percentage}}
        ],
        "regional_specificity": "more specific region if identifiable",
        "reasoning": "brief explanation of the analysis"
    }}
    
    Focus on vocabulary choices, spelling patterns, idiomatic expressions, and linguistic markers.
    Be specific about regional variants when possible.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert linguist specializing in accent and dialect identification from text. Always respond with valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=600
        )
        
        result = response.choices[0].message.content.strip()
        
        # Clean the response to ensure valid JSON
        if result.startswith('```json'):
            result = result[7:-3]
        elif result.startswith('```'):
            result = result[3:-3]
        
        try:
            parsed_result = json.loads(result)
            parsed_result['method'] = 'openai_gpt4'
            return parsed_result
        except json.JSONDecodeError:
            # Fallback parsing
            return {
                "primary_accent": "Analysis available",
                "confidence": 50,
                "key_indicators": [],
                "alternative_accents": [],
                "reasoning": result,
                "method": "openai_text_fallback"
            }
            
    except Exception as e:
        return {"error": f"OpenAI API error: {str(e)}"}

def phonetic_accent_analysis(text: str, language_code: str) -> Dict:
    """Analyze phonetic patterns that might indicate accent"""
    
    phonetic_indicators = {}
    
    if language_code.startswith('en'):
        # Look for phonetic spelling patterns
        patterns = {
            'r_dropping': re.findall(r'\b\w*ah\b|\b\w*eh\b', text.lower()),  # Non-rhotic accents
            'r_adding': re.findall(r'\bidear\b|\bsofa[hr]\b', text.lower()),  # Intrusive R
            'th_substitution': re.findall(r'\bdis\b|\bdat\b|\bdose\b', text.lower()),
            'vowel_shifts': re.findall(r'\bcah\b|\bpahk\b|\bbahth\b', text.lower()),  # Boston/NY patterns
        }
        
        phonetic_indicators = {k: len(v) for k, v in patterns.items() if v}
    
    return {
        'phonetic_patterns': phonetic_indicators,
        'method': 'phonetic_analysis'
    }

def enhanced_accent_detection(text: str, language_code: str) -> Dict:
    """Enhanced accent detection using multiple methods"""
    
    results = {
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'text_analyzed': text[:100] + "..." if len(text) > 100 else text,
        'language_code': language_code
    }
    
    # Method 1: Rule-based detection
    with st.spinner("Analyzing linguistic patterns..."):
        rule_based = detect_accent_patterns(text, language_code)
        results['rule_based'] = rule_based
    
    # Method 2: Phonetic analysis
    phonetic = phonetic_accent_analysis(text, language_code)
    results['phonetic'] = phonetic
    
    # Method 3: AI-powered analysis (if API key available)
    if OPENAI_API_KEY:
        with st.spinner("Getting AI accent analysis..."):
            ai_analysis = analyze_accent_with_openai(text, language_code)
            results['ai_analysis'] = ai_analysis
    else:
        results['ai_analysis'] = {"error": "OpenAI API key not configured"}
    
    # Combine results for final recommendation
    final_accent = "Unknown"
    final_confidence = 0
    reasoning = []
    
    # Priority: AI analysis > Rule-based > Default
    if 'ai_analysis' in results and 'primary_accent' in results['ai_analysis']:
        final_accent = results['ai_analysis']['primary_accent']
        final_confidence = results['ai_analysis'].get('confidence', 50)
        reasoning.append(f"AI analysis: {final_accent}")
    
    elif rule_based['detected_accent'] != 'Unknown':
        final_accent = rule_based['detected_accent']
        final_confidence = rule_based['confidence']
        reasoning.append(f"Pattern matching: {final_accent}")
    
    # Adjust confidence based on text length
    text_length_factor = min(len(text.split()) / 50, 1.0)  # Longer text = higher confidence
    final_confidence = int(final_confidence * text_length_factor)
    
    results['final_analysis'] = {
        'detected_accent': final_accent,
        'confidence': final_confidence,
        'reasoning': reasoning,
        'text_length_words': len(text.split()),
        'reliability': 'High' if final_confidence > 70 else 'Medium' if final_confidence > 40 else 'Low'
    }
    
    return results

def download_youtube_audio(url: str, max_duration: int = 600) -> str:
    """Download audio from YouTube video"""
    
    # Configure yt-dlp options
    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': '%(title)s.%(ext)s',
        'extractaudio': True,
        'audioformat': 'wav',
        'audioquality': '192K',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
            'preferredquality': '192',
        }],
        'quiet': True,
        'no_warnings': True,
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # Get video info first
            info = ydl.extract_info(url, download=False)
            duration = info.get('duration', 0)
            title = info.get('title', 'Unknown')
            
            # Check duration limit
            if duration > max_duration:
                raise Exception(f"Video too long ({duration}s). Maximum allowed: {max_duration}s")
            
            # Create temporary directory
            temp_dir = tempfile.mkdtemp()
            ydl_opts['outtmpl'] = os.path.join(temp_dir, '%(title)s.%(ext)s')
            
            # Download the video
            with yt_dlp.YoutubeDL(ydl_opts) as ydl_download:
                ydl_download.download([url])
            
            # Find the downloaded audio file
            audio_files = [f for f in os.listdir(temp_dir) if f.endswith('.wav')]
            if not audio_files:
                raise Exception("Failed to download audio file")
            
            audio_path = os.path.join(temp_dir, audio_files[0])
            
            return {
                'audio_path': audio_path,
                'title': title,
                'duration': duration,
                'temp_dir': temp_dir
            }
            
    except Exception as e:
        raise Exception(f"Failed to download YouTube audio: {str(e)}")

def transcribe_youtube_video(url: str, start_time: int = 0, duration: int = 60) -> Dict:
    """Download and transcribe YouTube video"""
    
    if not OPENAI_API_KEY:
        raise Exception("OpenAI API key required for transcription")
    
    try:
        # Download audio
        with st.spinner("Downloading YouTube audio..."):
            download_result = download_youtube_audio(url)
        
        # Transcribe with OpenAI Whisper
        with st.spinner("Transcribing audio..."):
            client = openai.OpenAI(api_key=OPENAI_API_KEY)
            
            with open(download_result['audio_path'], "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    response_format="verbose_json",
                    language=None,
                    temperature=0
                )
        
        # Clean up temporary files
        try:
            os.remove(download_result['audio_path'])
            os.rmdir(download_result['temp_dir'])
        except:
            pass  # Ignore cleanup errors
        
        return {
            "text": transcript.text,
            "language": getattr(transcript, 'language', 'unknown'),
            "confidence": 0.9,
            "duration": download_result['duration'],
            "title": download_result['title'],
            "url": url,
            "method": "youtube_whisper"
        }
        
    except Exception as e:
        raise Exception(f"YouTube transcription failed: {str(e)}")

def is_valid_youtube_url(url: str) -> bool:
    """Check if URL is a valid YouTube URL"""
    youtube_patterns = [
        r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=[\w-]+',
        r'(?:https?://)?(?:www\.)?youtu\.be/[\w-]+',
        r'(?:https?://)?(?:m\.)?youtube\.com/watch\?v=[\w-]+',
    ]
    
    return any(re.match(pattern, url) for pattern in youtube_patterns)
    """Transcribe audio using OpenAI Whisper API"""
    
    if not OPENAI_API_KEY:
        raise Exception("OpenAI API key required for Whisper transcription")
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    
    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_file.read())
            tmp_file_path = tmp_file.name
        
        try:
            with open(tmp_file_path, "rb") as audio_file_obj:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file_obj,
                    response_format="verbose_json",  # Get more detailed response
                    language=None,  # Auto-detect language
                    temperature=0  # More deterministic results
                )
            
            # Extract language information
            detected_language = getattr(transcript, 'language', 'unknown')
            
            return {
                "text": transcript.text,
                "language": detected_language,
                "confidence": 0.9,  # Whisper doesn't provide confidence scores
                "duration": getattr(transcript, 'duration', 0),
                "segments": getattr(transcript, 'segments', []),
                "method": "openai_whisper"
            }
        finally:
            # Clean up temporary file
            os.unlink(tmp_file_path)
        
    except Exception as e:
        raise Exception(f"OpenAI Whisper transcription failed: {str(e)}")

def display_accent_results(results: Dict):
    """Display accent detection results in Streamlit"""
    
    st.subheader("üé≠ Accent Analysis Results")
    
    # Final analysis
    final = results.get('final_analysis', {})
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Detected Accent", final.get('detected_accent', 'Unknown'))
    with col2:
        st.metric("Confidence", f"{final.get('confidence', 0)}%")
    with col3:
        st.metric("Reliability", final.get('reliability', 'Unknown'))
    
    # Text analysis info
    st.info(f"üìù Analyzed {final.get('text_length_words', 0)} words at {results.get('timestamp', 'unknown time')}")
    
    # Detailed breakdown
    with st.expander("üìä Detailed Analysis", expanded=False):
        
        # Rule-based results
        if 'rule_based' in results:
            st.write("**Pattern-Based Analysis:**")
            rb = results['rule_based']
            if rb['detected_accent'] != 'Unknown':
                st.write(f"- Detected: {rb['detected_accent']} ({rb['confidence']}%)")
                if rb['features_found']:
                    st.write("- Key features found:")
                    for feature in rb['features_found']:
                        st.markdown(f"  <span class='feature-found'>{feature}</span>", unsafe_allow_html=True)
            else:
                st.write("- No clear patterns detected")
        
        # AI analysis results
        if 'ai_analysis' in results and 'primary_accent' in results['ai_analysis']:
            st.write("**AI Analysis:**")
            ai = results['ai_analysis']
            st.write(f"- Primary: {ai['primary_accent']} ({ai.get('confidence', 0)}%)")
            
            if ai.get('key_indicators'):
                st.write(f"- Indicators: {', '.join(ai['key_indicators'])}")
            
            if ai.get('alternative_accents'):
                st.write("- Alternatives:")
                for alt in ai['alternative_accents'][:3]:  # Show top 3
                    st.write(f"  - {alt.get('accent', 'Unknown')} ({alt.get('probability', 0)}%)")
            
            if ai.get('reasoning'):
                st.write(f"- Reasoning: {ai['reasoning']}")
        
        # Phonetic analysis
        if 'phonetic' in results and results['phonetic']['phonetic_patterns']:
            st.write("**Phonetic Patterns:**")
            for pattern, count in results['phonetic']['phonetic_patterns'].items():
                st.write(f"- {pattern.replace('_', ' ').title()}: {count} occurrences")

def main():
    """Main Streamlit application"""
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üé≠ Accent Detection App</h1>
        <p>Analyze accents and dialects from text or speech using AI</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # API Key status
        if OPENAI_API_KEY:
            st.success("‚úÖ OpenAI API Key configured")
        else:
            st.error("‚ùå OpenAI API Key missing")
            st.info("Add your OpenAI API key to Streamlit secrets to enable AI analysis and audio transcription.")
        
        # Language selection
        language_options = {
            "English": "en-US",
            "Spanish": "es-ES", 
            "French": "fr-FR",
            "German": "de-DE",
            "Italian": "it-IT",
            "Portuguese": "pt-PT"
        }
        
        selected_language = st.selectbox(
            "Select Language:",
            list(language_options.keys())
        )
        language_code = language_options[selected_language]
        
        st.divider()
        
        # Sample texts
        st.subheader("üìù Sample Texts")
        sample_texts = {
            "American English": "I'm gonna grab some coffee and candy from the elevator, then head to my apartment. Y'all want anything?",
            "British English": "I'll pop to the lift and get some sweets, then head back to my flat. Cheers mate, brilliant!",
            "Australian English": "G'day mate! Let's have a barbie this arvo down at the servo, it'll be bonzer! No worries!",
            "Canadian English": "It's aboot time we head to the washroom, eh? Don't forget your toque, it's cold out there!",
            "Mexican Spanish": "¬°√ìrale g√ºey! Vamos a echar la chela, est√° muy chido el ambiente aqu√≠, ¬øno?",
            "French (France)": "Salut ! On va prendre un caf√© au bistrot, c'est chouette cette ambiance, non ?",
        }
        
        for accent, text in sample_texts.items():
            if st.button(f"Use {accent} sample", key=f"sample_{accent}"):
                st.session_state.sample_text = text
    
    # Main content tabs
    tab1, tab2, tab3 = st.tabs(["üìù Text Analysis", "üéôÔ∏è Audio Analysis", "üì∫ YouTube Analysis"])
    
    with tab1:
        st.header("Text-based Accent Detection")
        
        # Text input
        text_input = st.text_area(
            "Enter text to analyze:",
            value=st.session_state.get('sample_text', ''),
            height=150,
            placeholder="Type or paste text here..."
        )
        
        col1, col2 = st.columns([3, 1])
        with col1:
            analyze_button = st.button("üîç Analyze Accent", type="primary", use_container_width=True)
        with col2:
            clear_button = st.button("üóëÔ∏è Clear", use_container_width=True)
        
        if clear_button:
            st.session_state.sample_text = ""
            st.rerun()
        
        if analyze_button:
            if text_input.strip():
                with st.spinner("Analyzing accent..."):
                    results = enhanced_accent_detection(text_input, language_code)
                    display_accent_results(results)
            else:
                st.warning("‚ö†Ô∏è Please enter some text to analyze.")
    
    with tab2:
        st.header("Audio-based Accent Detection")
        
        if not OPENAI_API_KEY:
            st.warning("‚ö†Ô∏è OpenAI API key required for audio transcription. Please add it to your Streamlit secrets.")
        else:
            st.info("üé§ Upload an audio file to transcribe and analyze the accent.")
            
            uploaded_file = st.file_uploader(
                "Choose an audio file",
                type=['wav', 'mp3', 'm4a', 'ogg', 'flac'],
                help="Supported formats: WAV, MP3, M4A, OGG, FLAC"
            )
            
            if uploaded_file is not None:
                st.audio(uploaded_file, format='audio/wav')
                
                if st.button("üéôÔ∏è Transcribe & Analyze", type="primary"):
                    try:
                        with st.spinner("Transcribing audio..."):
                            transcription_result = transcribe_with_openai(uploaded_file)
                        
                        st.success("‚úÖ Transcription completed!")
                        
                        # Display transcription
                        st.subheader("üìù Transcription")
                        st.write(f"**Detected Language:** {transcription_result.get('language', 'Unknown')}")
                        st.write(f"**Text:** {transcription_result.get('text', '')}")
                        
                        # Analyze accent from transcription
                        if transcription_result.get('text'):
                            with st.spinner("Analyzing accent from transcription..."):
                                accent_results = enhanced_accent_detection(
                                    transcription_result['text'], 
                                    transcription_result.get('language', language_code)
                                )
                                display_accent_results(accent_results)
                    
                    except Exception as e:
                        st.error(f"‚ùå Error processing audio: {str(e)}")

    with tab3:
        st.header("YouTube Video Accent Detection")
        
        if not OPENAI_API_KEY:
            st.warning("‚ö†Ô∏è OpenAI API key required for YouTube video transcription. Please add it to your Streamlit secrets.")
        else:
            st.info("üì∫ Enter a YouTube URL to download, transcribe, and analyze the accent.")
            
            # YouTube URL input
            youtube_url = st.text_input(
                "YouTube URL:",
                placeholder="https://www.youtube.com/watch?v=...",
                help="Enter a YouTube video URL (max 10 minutes for processing efficiency)"
            )
            
            col1, col2 = st.columns(2)
            with col1:
                max_duration = st.slider(
                    "Max video duration (seconds):",
                    min_value=30,
                    max_value=600,
                    value=300,
                    step=30,
                    help="Longer videos take more time and credits to process"
                )
            
            with col2:
                st.write("")  # Spacing
                st.write("")  # Spacing
                process_youtube = st.button("üì∫ Process YouTube Video", type="primary")
            
            if process_youtube:
                if not youtube_url:
                    st.error("‚ùå Please enter a YouTube URL")
                elif not is_valid_youtube_url(youtube_url):
                    st.error("‚ùå Please enter a valid YouTube URL")
                else:
                    try:
                        # Process YouTube video
                        with st.spinner("Processing YouTube video..."):
                            transcription_result = transcribe_youtube_video(youtube_url)
                        
                        st.success("‚úÖ YouTube video processed successfully!")
                        
                        # Display video info
                        st.subheader("üì∫ Video Information")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**Title:** {transcription_result.get('title', 'Unknown')}")
                            st.write(f"**Duration:** {transcription_result.get('duration', 0)} seconds")
                        with col2:
                            st.write(f"**Detected Language:** {transcription_result.get('language', 'Unknown')}")
                            st.write(f"**URL:** [Link]({youtube_url})")
                        
                        # Display transcription
                        st.subheader("üìù Transcription")
                        transcription_text = transcription_result.get('text', '')
                        
                        if len(transcription_text) > 500:
                            with st.expander("Show full transcription", expanded=False):
                                st.write(transcription_text)
                            st.write(f"**Preview:** {transcription_text[:500]}...")
                        else:
                            st.write(transcription_text)
                        
                        # Analyze accent from transcription
                        if transcription_text:
                            with st.spinner("Analyzing accent from YouTube video..."):
                                accent_results = enhanced_accent_detection(
                                    transcription_text, 
                                    transcription_result.get('language', language_code)
                                )
                                display_accent_results(accent_results)
                        
                    except Exception as e:
                        st.error(f"‚ùå Error processing YouTube video: {str(e)}")
                        
                        # Common error suggestions
                        error_msg = str(e).lower()
                        if "private" in error_msg or "unavailable" in error_msg:
                            st.info("üí° Try a different public YouTube video")
                        elif "long" in error_msg:
                            st.info("üí° Try a shorter video or increase the duration limit")
                        elif "download" in error_msg:
                            st.info("üí° Check your internet connection and try again")
            
            # Example YouTube videos
            st.subheader("üé¨ Example Videos")
            st.write("Try these example videos to test the accent detection:")
            
            example_videos = {
                "American English": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # Replace with actual examples
                "British English": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
                "Australian English": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
            }
            
            for accent, url in example_videos.items():
                if st.button(f"Use {accent} example", key=f"yt_sample_{accent}"):
                    st.session_state.youtube_url = url

    # Footer
    st.divider()
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 1rem;">
        <p>üé≠ Accent Detection App | Powered by OpenAI GPT-4 & Whisper</p>
        <p><small>Analyzes linguistic patterns, vocabulary, and regional markers to identify accents and dialects</small></p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
